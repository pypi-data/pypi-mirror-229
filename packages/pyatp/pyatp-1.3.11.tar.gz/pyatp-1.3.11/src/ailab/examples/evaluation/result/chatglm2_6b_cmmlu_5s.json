{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.42011834319526625,
      "acc_stderr": 0.03808034433196808,
      "acc_norm": 0.42011834319526625,
      "acc_norm_stderr": 0.03808034433196808
    },
    "Cmmlu-anatomy": {
      "acc": 0.36486486486486486,
      "acc_stderr": 0.039704563322785984,
      "acc_norm": 0.36486486486486486,
      "acc_norm_stderr": 0.039704563322785984
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.29878048780487804,
      "acc_stderr": 0.03585166336909661,
      "acc_norm": 0.29878048780487804,
      "acc_norm_stderr": 0.03585166336909661
    },
    "Cmmlu-arts": {
      "acc": 0.675,
      "acc_stderr": 0.03714454174077365,
      "acc_norm": 0.675,
      "acc_norm_stderr": 0.03714454174077365
    },
    "Cmmlu-astronomy": {
      "acc": 0.2909090909090909,
      "acc_stderr": 0.03546563019624336,
      "acc_norm": 0.2909090909090909,
      "acc_norm_stderr": 0.03546563019624336
    },
    "Cmmlu-business_ethics": {
      "acc": 0.46411483253588515,
      "acc_stderr": 0.03457935791797729,
      "acc_norm": 0.46411483253588515,
      "acc_norm_stderr": 0.03457935791797729
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.50625,
      "acc_stderr": 0.03964948130713094,
      "acc_norm": 0.50625,
      "acc_norm_stderr": 0.03964948130713094
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.5954198473282443,
      "acc_stderr": 0.043046937953806645,
      "acc_norm": 0.5954198473282443,
      "acc_norm_stderr": 0.043046937953806645
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.4264705882352941,
      "acc_stderr": 0.04256528107076941,
      "acc_norm": 0.4264705882352941,
      "acc_norm_stderr": 0.04256528107076941
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.5700934579439252,
      "acc_stderr": 0.04808472349429953,
      "acc_norm": 0.5700934579439252,
      "acc_norm_stderr": 0.04808472349429953
    },
    "Cmmlu-chinese_history": {
      "acc": 0.6470588235294118,
      "acc_stderr": 0.02663146824134736,
      "acc_norm": 0.6470588235294118,
      "acc_norm_stderr": 0.02663146824134736
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.4019607843137255,
      "acc_stderr": 0.034411900234824655,
      "acc_norm": 0.4019607843137255,
      "acc_norm_stderr": 0.034411900234824655
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.5865921787709497,
      "acc_stderr": 0.03691029168738378,
      "acc_norm": 0.5865921787709497,
      "acc_norm_stderr": 0.03691029168738378
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.4050632911392405,
      "acc_stderr": 0.031955147413706725,
      "acc_norm": 0.4050632911392405,
      "acc_norm_stderr": 0.031955147413706725
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.2830188679245283,
      "acc_stderr": 0.04396093377439376,
      "acc_norm": 0.2830188679245283,
      "acc_norm_stderr": 0.04396093377439376
    },
    "Cmmlu-college_education": {
      "acc": 0.6728971962616822,
      "acc_stderr": 0.04556837693674773,
      "acc_norm": 0.6728971962616822,
      "acc_norm_stderr": 0.04556837693674773
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.39622641509433965,
      "acc_stderr": 0.047732492983673595,
      "acc_norm": 0.39622641509433965,
      "acc_norm_stderr": 0.047732492983673595
    },
    "Cmmlu-college_law": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.04750077341199984,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.04750077341199984
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.3047619047619048,
      "acc_stderr": 0.0451367671816831,
      "acc_norm": 0.3047619047619048,
      "acc_norm_stderr": 0.0451367671816831
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.44339622641509435,
      "acc_stderr": 0.048481318229754794,
      "acc_norm": 0.44339622641509435,
      "acc_norm_stderr": 0.048481318229754794
    },
    "Cmmlu-college_medicine": {
      "acc": 0.4432234432234432,
      "acc_stderr": 0.030120860870184646,
      "acc_norm": 0.4432234432234432,
      "acc_norm_stderr": 0.030120860870184646
    },
    "Cmmlu-computer_science": {
      "acc": 0.45588235294117646,
      "acc_stderr": 0.034956245220154725,
      "acc_norm": 0.45588235294117646,
      "acc_norm_stderr": 0.034956245220154725
    },
    "Cmmlu-computer_security": {
      "acc": 0.5263157894736842,
      "acc_stderr": 0.038295098689947266,
      "acc_norm": 0.5263157894736842,
      "acc_norm_stderr": 0.038295098689947266
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.6598639455782312,
      "acc_stderr": 0.0392082182208768,
      "acc_norm": 0.6598639455782312,
      "acc_norm_stderr": 0.0392082182208768
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.39568345323741005,
      "acc_stderr": 0.04162618828625744,
      "acc_norm": 0.39568345323741005,
      "acc_norm_stderr": 0.04162618828625744
    },
    "Cmmlu-economics": {
      "acc": 0.44654088050314467,
      "acc_stderr": 0.03954985017675704,
      "acc_norm": 0.44654088050314467,
      "acc_norm_stderr": 0.03954985017675704
    },
    "Cmmlu-education": {
      "acc": 0.6319018404907976,
      "acc_stderr": 0.03789213935838396,
      "acc_norm": 0.6319018404907976,
      "acc_norm_stderr": 0.03789213935838396
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.38953488372093026,
      "acc_stderr": 0.03729113044487178,
      "acc_norm": 0.38953488372093026,
      "acc_norm_stderr": 0.03729113044487178
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.4642857142857143,
      "acc_stderr": 0.03147910771121848,
      "acc_norm": 0.4642857142857143,
      "acc_norm_stderr": 0.03147910771121848
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.5050505050505051,
      "acc_stderr": 0.035621707606254015,
      "acc_norm": 0.5050505050505051,
      "acc_norm_stderr": 0.035621707606254015
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.6260504201680672,
      "acc_stderr": 0.031429466378837076,
      "acc_norm": 0.6260504201680672,
      "acc_norm_stderr": 0.031429466378837076
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.36086956521739133,
      "acc_stderr": 0.03173599626649655,
      "acc_norm": 0.36086956521739133,
      "acc_norm_stderr": 0.03173599626649655
    },
    "Cmmlu-ethnology": {
      "acc": 0.37037037037037035,
      "acc_stderr": 0.041716541613545426,
      "acc_norm": 0.37037037037037035,
      "acc_norm_stderr": 0.041716541613545426
    },
    "Cmmlu-food_science": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.04178532361608381,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.04178532361608381
    },
    "Cmmlu-genetics": {
      "acc": 0.4147727272727273,
      "acc_stderr": 0.03724331671462075,
      "acc_norm": 0.4147727272727273,
      "acc_norm_stderr": 0.03724331671462075
    },
    "Cmmlu-global_facts": {
      "acc": 0.4899328859060403,
      "acc_stderr": 0.041091415327375716,
      "acc_norm": 0.4899328859060403,
      "acc_norm_stderr": 0.041091415327375716
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.6804733727810651,
      "acc_stderr": 0.03597530251676528,
      "acc_norm": 0.6804733727810651,
      "acc_norm_stderr": 0.03597530251676528
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.5606060606060606,
      "acc_stderr": 0.04336309556090911,
      "acc_norm": 0.5606060606060606,
      "acc_norm_stderr": 0.04336309556090911
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.5338983050847458,
      "acc_stderr": 0.04611866011948887,
      "acc_norm": 0.5338983050847458,
      "acc_norm_stderr": 0.04611866011948887
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.3170731707317073,
      "acc_stderr": 0.03644794381282878,
      "acc_norm": 0.3170731707317073,
      "acc_norm_stderr": 0.03644794381282878
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.04769300568972743,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.04769300568972743
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.5314685314685315,
      "acc_stderr": 0.041875883974458974,
      "acc_norm": 0.5314685314685315,
      "acc_norm_stderr": 0.041875883974458974
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.0442626668137991,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.0442626668137991
    },
    "Cmmlu-international_law": {
      "acc": 0.34594594594594597,
      "acc_stderr": 0.03506727605846201,
      "acc_norm": 0.34594594594594597,
      "acc_norm_stderr": 0.03506727605846201
    },
    "Cmmlu-journalism": {
      "acc": 0.47093023255813954,
      "acc_stderr": 0.0381712782490057,
      "acc_norm": 0.47093023255813954,
      "acc_norm_stderr": 0.0381712782490057
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.5036496350364964,
      "acc_stderr": 0.024692582087670466,
      "acc_norm": 0.5036496350364964,
      "acc_norm_stderr": 0.024692582087670466
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.8364485981308412,
      "acc_stderr": 0.02534293808681739,
      "acc_norm": 0.8364485981308412,
      "acc_norm_stderr": 0.02534293808681739
    },
    "Cmmlu-logical": {
      "acc": 0.35772357723577236,
      "acc_stderr": 0.04339651526440302,
      "acc_norm": 0.35772357723577236,
      "acc_norm_stderr": 0.04339651526440302
    },
    "Cmmlu-machine_learning": {
      "acc": 0.4344262295081967,
      "acc_stderr": 0.04506194823469704,
      "acc_norm": 0.4344262295081967,
      "acc_norm_stderr": 0.04506194823469704
    },
    "Cmmlu-management": {
      "acc": 0.5952380952380952,
      "acc_stderr": 0.0339525213962775,
      "acc_norm": 0.5952380952380952,
      "acc_norm_stderr": 0.0339525213962775
    },
    "Cmmlu-marketing": {
      "acc": 0.5666666666666667,
      "acc_stderr": 0.037038071576695496,
      "acc_norm": 0.5666666666666667,
      "acc_norm_stderr": 0.037038071576695496
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.5767195767195767,
      "acc_stderr": 0.03603441813251289,
      "acc_norm": 0.5767195767195767,
      "acc_norm_stderr": 0.03603441813251289
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3706896551724138,
      "acc_stderr": 0.0450390009465778,
      "acc_norm": 0.3706896551724138,
      "acc_norm_stderr": 0.0450390009465778
    },
    "Cmmlu-nutrition": {
      "acc": 0.47586206896551725,
      "acc_stderr": 0.041618085035015295,
      "acc_norm": 0.47586206896551725,
      "acc_norm_stderr": 0.041618085035015295
    },
    "Cmmlu-philosophy": {
      "acc": 0.6,
      "acc_stderr": 0.04803844614152614,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.04803844614152614
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.5485714285714286,
      "acc_stderr": 0.037725628985298354,
      "acc_norm": 0.5485714285714286,
      "acc_norm_stderr": 0.037725628985298354
    },
    "Cmmlu-professional_law": {
      "acc": 0.38388625592417064,
      "acc_stderr": 0.033560010105331634,
      "acc_norm": 0.38388625592417064,
      "acc_norm_stderr": 0.033560010105331634
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.3537234042553192,
      "acc_stderr": 0.02469024949447835,
      "acc_norm": 0.3537234042553192,
      "acc_norm_stderr": 0.02469024949447835
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.5560344827586207,
      "acc_stderr": 0.03269034414952844,
      "acc_norm": 0.5560344827586207,
      "acc_norm_stderr": 0.03269034414952844
    },
    "Cmmlu-public_relations": {
      "acc": 0.5229885057471264,
      "acc_stderr": 0.03797409587134035,
      "acc_norm": 0.5229885057471264,
      "acc_norm_stderr": 0.03797409587134035
    },
    "Cmmlu-security_study": {
      "acc": 0.6148148148148148,
      "acc_stderr": 0.042039210401562783,
      "acc_norm": 0.6148148148148148,
      "acc_norm_stderr": 0.042039210401562783
    },
    "Cmmlu-sociology": {
      "acc": 0.4823008849557522,
      "acc_stderr": 0.0333124428756083,
      "acc_norm": 0.4823008849557522,
      "acc_norm_stderr": 0.0333124428756083
    },
    "Cmmlu-sports_science": {
      "acc": 0.48484848484848486,
      "acc_stderr": 0.03902551007374448,
      "acc_norm": 0.48484848484848486,
      "acc_norm_stderr": 0.03902551007374448
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.4810810810810811,
      "acc_stderr": 0.036834092970087065,
      "acc_norm": 0.4810810810810811,
      "acc_norm_stderr": 0.036834092970087065
    },
    "Cmmlu-virology": {
      "acc": 0.47337278106508873,
      "acc_stderr": 0.03852109743620031,
      "acc_norm": 0.47337278106508873,
      "acc_norm_stderr": 0.03852109743620031
    },
    "Cmmlu-world_history": {
      "acc": 0.5838509316770186,
      "acc_stderr": 0.03896865898200244,
      "acc_norm": 0.5838509316770186,
      "acc_norm_stderr": 0.03896865898200244
    },
    "Cmmlu-world_religions": {
      "acc": 0.53125,
      "acc_stderr": 0.039575057062617526,
      "acc_norm": 0.53125,
      "acc_norm_stderr": 0.039575057062617526
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-chatglm",
    "model_args": "pretrained=/home/sdk_models/chatglm2_6b,add_special_tokens=True,dtype='float16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:08:32.716048",
    "model_name": "chatglm2_6b"
  }
}