{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.4260355029585799,
      "acc_stderr": 0.038151425516134464,
      "acc_norm": 0.4260355029585799,
      "acc_norm_stderr": 0.038151425516134464
    },
    "Cmmlu-anatomy": {
      "acc": 0.36486486486486486,
      "acc_stderr": 0.039704563322785984,
      "acc_norm": 0.36486486486486486,
      "acc_norm_stderr": 0.039704563322785984
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.2926829268292683,
      "acc_stderr": 0.03563788836258829,
      "acc_norm": 0.2926829268292683,
      "acc_norm_stderr": 0.03563788836258829
    },
    "Cmmlu-arts": {
      "acc": 0.66875,
      "acc_stderr": 0.03732598513993524,
      "acc_norm": 0.66875,
      "acc_norm_stderr": 0.03732598513993524
    },
    "Cmmlu-astronomy": {
      "acc": 0.2909090909090909,
      "acc_stderr": 0.03546563019624336,
      "acc_norm": 0.2909090909090909,
      "acc_norm_stderr": 0.03546563019624336
    },
    "Cmmlu-business_ethics": {
      "acc": 0.4688995215311005,
      "acc_stderr": 0.034601631258720345,
      "acc_norm": 0.4688995215311005,
      "acc_norm_stderr": 0.034601631258720345
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.50625,
      "acc_stderr": 0.03964948130713094,
      "acc_norm": 0.50625,
      "acc_norm_stderr": 0.03964948130713094
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.5954198473282443,
      "acc_stderr": 0.043046937953806645,
      "acc_norm": 0.5954198473282443,
      "acc_norm_stderr": 0.043046937953806645
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.41911764705882354,
      "acc_stderr": 0.04246637405992851,
      "acc_norm": 0.41911764705882354,
      "acc_norm_stderr": 0.04246637405992851
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.5700934579439252,
      "acc_stderr": 0.04808472349429953,
      "acc_norm": 0.5700934579439252,
      "acc_norm_stderr": 0.04808472349429953
    },
    "Cmmlu-chinese_history": {
      "acc": 0.6470588235294118,
      "acc_stderr": 0.02663146824134736,
      "acc_norm": 0.6470588235294118,
      "acc_norm_stderr": 0.02663146824134736
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.4019607843137255,
      "acc_stderr": 0.034411900234824655,
      "acc_norm": 0.4019607843137255,
      "acc_norm_stderr": 0.034411900234824655
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.5865921787709497,
      "acc_stderr": 0.03691029168738378,
      "acc_norm": 0.5865921787709497,
      "acc_norm_stderr": 0.03691029168738378
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.4092827004219409,
      "acc_stderr": 0.03200704183359591,
      "acc_norm": 0.4092827004219409,
      "acc_norm_stderr": 0.03200704183359591
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.29245283018867924,
      "acc_stderr": 0.044392639061996274,
      "acc_norm": 0.29245283018867924,
      "acc_norm_stderr": 0.044392639061996274
    },
    "Cmmlu-college_education": {
      "acc": 0.6635514018691588,
      "acc_stderr": 0.04589271111471628,
      "acc_norm": 0.6635514018691588,
      "acc_norm_stderr": 0.04589271111471628
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.39622641509433965,
      "acc_stderr": 0.047732492983673595,
      "acc_norm": 0.39622641509433965,
      "acc_norm_stderr": 0.047732492983673595
    },
    "Cmmlu-college_law": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.04750077341199984,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.04750077341199984
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.3047619047619048,
      "acc_stderr": 0.0451367671816831,
      "acc_norm": 0.3047619047619048,
      "acc_norm_stderr": 0.0451367671816831
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.44339622641509435,
      "acc_stderr": 0.048481318229754794,
      "acc_norm": 0.44339622641509435,
      "acc_norm_stderr": 0.048481318229754794
    },
    "Cmmlu-college_medicine": {
      "acc": 0.43956043956043955,
      "acc_stderr": 0.03009464601676741,
      "acc_norm": 0.43956043956043955,
      "acc_norm_stderr": 0.03009464601676741
    },
    "Cmmlu-computer_science": {
      "acc": 0.45588235294117646,
      "acc_stderr": 0.034956245220154725,
      "acc_norm": 0.45588235294117646,
      "acc_norm_stderr": 0.034956245220154725
    },
    "Cmmlu-computer_security": {
      "acc": 0.5263157894736842,
      "acc_stderr": 0.038295098689947266,
      "acc_norm": 0.5263157894736842,
      "acc_norm_stderr": 0.038295098689947266
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.6598639455782312,
      "acc_stderr": 0.0392082182208768,
      "acc_norm": 0.6598639455782312,
      "acc_norm_stderr": 0.0392082182208768
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.39568345323741005,
      "acc_stderr": 0.04162618828625744,
      "acc_norm": 0.39568345323741005,
      "acc_norm_stderr": 0.04162618828625744
    },
    "Cmmlu-economics": {
      "acc": 0.44654088050314467,
      "acc_stderr": 0.03954985017675704,
      "acc_norm": 0.44654088050314467,
      "acc_norm_stderr": 0.03954985017675704
    },
    "Cmmlu-education": {
      "acc": 0.6319018404907976,
      "acc_stderr": 0.03789213935838396,
      "acc_norm": 0.6319018404907976,
      "acc_norm_stderr": 0.03789213935838396
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.38953488372093026,
      "acc_stderr": 0.03729113044487178,
      "acc_norm": 0.38953488372093026,
      "acc_norm_stderr": 0.03729113044487178
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.46825396825396826,
      "acc_stderr": 0.03149604347936578,
      "acc_norm": 0.46825396825396826,
      "acc_norm_stderr": 0.03149604347936578
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.5050505050505051,
      "acc_stderr": 0.035621707606254015,
      "acc_norm": 0.5050505050505051,
      "acc_norm_stderr": 0.035621707606254015
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.6260504201680672,
      "acc_stderr": 0.031429466378837076,
      "acc_norm": 0.6260504201680672,
      "acc_norm_stderr": 0.031429466378837076
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.3521739130434783,
      "acc_stderr": 0.03156385695012927,
      "acc_norm": 0.3521739130434783,
      "acc_norm_stderr": 0.03156385695012927
    },
    "Cmmlu-ethnology": {
      "acc": 0.37037037037037035,
      "acc_stderr": 0.041716541613545426,
      "acc_norm": 0.37037037037037035,
      "acc_norm_stderr": 0.041716541613545426
    },
    "Cmmlu-food_science": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.04178532361608381,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.04178532361608381
    },
    "Cmmlu-genetics": {
      "acc": 0.42045454545454547,
      "acc_stderr": 0.037315069392646734,
      "acc_norm": 0.42045454545454547,
      "acc_norm_stderr": 0.037315069392646734
    },
    "Cmmlu-global_facts": {
      "acc": 0.4899328859060403,
      "acc_stderr": 0.041091415327375716,
      "acc_norm": 0.4899328859060403,
      "acc_norm_stderr": 0.041091415327375716
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.6804733727810651,
      "acc_stderr": 0.03597530251676528,
      "acc_norm": 0.6804733727810651,
      "acc_norm_stderr": 0.03597530251676528
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.5606060606060606,
      "acc_stderr": 0.04336309556090911,
      "acc_norm": 0.5606060606060606,
      "acc_norm_stderr": 0.04336309556090911
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.5338983050847458,
      "acc_stderr": 0.04611866011948887,
      "acc_norm": 0.5338983050847458,
      "acc_norm_stderr": 0.04611866011948887
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.3231707317073171,
      "acc_stderr": 0.036632096446028446,
      "acc_norm": 0.3231707317073171,
      "acc_norm_stderr": 0.036632096446028446
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.04769300568972743,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.04769300568972743
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.5314685314685315,
      "acc_stderr": 0.041875883974458974,
      "acc_norm": 0.5314685314685315,
      "acc_norm_stderr": 0.041875883974458974
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.0442626668137991,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.0442626668137991
    },
    "Cmmlu-international_law": {
      "acc": 0.34594594594594597,
      "acc_stderr": 0.03506727605846201,
      "acc_norm": 0.34594594594594597,
      "acc_norm_stderr": 0.03506727605846201
    },
    "Cmmlu-journalism": {
      "acc": 0.47093023255813954,
      "acc_stderr": 0.0381712782490057,
      "acc_norm": 0.47093023255813954,
      "acc_norm_stderr": 0.0381712782490057
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.5060827250608273,
      "acc_stderr": 0.02469141257138466,
      "acc_norm": 0.5060827250608273,
      "acc_norm_stderr": 0.02469141257138466
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.8364485981308412,
      "acc_stderr": 0.02534293808681739,
      "acc_norm": 0.8364485981308412,
      "acc_norm_stderr": 0.02534293808681739
    },
    "Cmmlu-logical": {
      "acc": 0.35772357723577236,
      "acc_stderr": 0.04339651526440302,
      "acc_norm": 0.35772357723577236,
      "acc_norm_stderr": 0.04339651526440302
    },
    "Cmmlu-machine_learning": {
      "acc": 0.4344262295081967,
      "acc_stderr": 0.04506194823469704,
      "acc_norm": 0.4344262295081967,
      "acc_norm_stderr": 0.04506194823469704
    },
    "Cmmlu-management": {
      "acc": 0.5904761904761905,
      "acc_stderr": 0.03401477718256437,
      "acc_norm": 0.5904761904761905,
      "acc_norm_stderr": 0.03401477718256437
    },
    "Cmmlu-marketing": {
      "acc": 0.5611111111111111,
      "acc_stderr": 0.0370915696198558,
      "acc_norm": 0.5611111111111111,
      "acc_norm_stderr": 0.0370915696198558
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.5767195767195767,
      "acc_stderr": 0.03603441813251289,
      "acc_norm": 0.5767195767195767,
      "acc_norm_stderr": 0.03603441813251289
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3706896551724138,
      "acc_stderr": 0.0450390009465778,
      "acc_norm": 0.3706896551724138,
      "acc_norm_stderr": 0.0450390009465778
    },
    "Cmmlu-nutrition": {
      "acc": 0.47586206896551725,
      "acc_stderr": 0.041618085035015295,
      "acc_norm": 0.47586206896551725,
      "acc_norm_stderr": 0.041618085035015295
    },
    "Cmmlu-philosophy": {
      "acc": 0.6,
      "acc_stderr": 0.04803844614152614,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.04803844614152614
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.5542857142857143,
      "acc_stderr": 0.03768083305144796,
      "acc_norm": 0.5542857142857143,
      "acc_norm_stderr": 0.03768083305144796
    },
    "Cmmlu-professional_law": {
      "acc": 0.38388625592417064,
      "acc_stderr": 0.033560010105331634,
      "acc_norm": 0.38388625592417064,
      "acc_norm_stderr": 0.033560010105331634
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.3537234042553192,
      "acc_stderr": 0.02469024949447835,
      "acc_norm": 0.3537234042553192,
      "acc_norm_stderr": 0.02469024949447835
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.5560344827586207,
      "acc_stderr": 0.03269034414952844,
      "acc_norm": 0.5560344827586207,
      "acc_norm_stderr": 0.03269034414952844
    },
    "Cmmlu-public_relations": {
      "acc": 0.5229885057471264,
      "acc_stderr": 0.03797409587134035,
      "acc_norm": 0.5229885057471264,
      "acc_norm_stderr": 0.03797409587134035
    },
    "Cmmlu-security_study": {
      "acc": 0.6148148148148148,
      "acc_stderr": 0.042039210401562783,
      "acc_norm": 0.6148148148148148,
      "acc_norm_stderr": 0.042039210401562783
    },
    "Cmmlu-sociology": {
      "acc": 0.4823008849557522,
      "acc_stderr": 0.0333124428756083,
      "acc_norm": 0.4823008849557522,
      "acc_norm_stderr": 0.0333124428756083
    },
    "Cmmlu-sports_science": {
      "acc": 0.48484848484848486,
      "acc_stderr": 0.03902551007374448,
      "acc_norm": 0.48484848484848486,
      "acc_norm_stderr": 0.03902551007374448
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.4810810810810811,
      "acc_stderr": 0.036834092970087065,
      "acc_norm": 0.4810810810810811,
      "acc_norm_stderr": 0.036834092970087065
    },
    "Cmmlu-virology": {
      "acc": 0.47337278106508873,
      "acc_stderr": 0.03852109743620031,
      "acc_norm": 0.47337278106508873,
      "acc_norm_stderr": 0.03852109743620031
    },
    "Cmmlu-world_history": {
      "acc": 0.5838509316770186,
      "acc_stderr": 0.03896865898200244,
      "acc_norm": 0.5838509316770186,
      "acc_norm_stderr": 0.03896865898200244
    },
    "Cmmlu-world_religions": {
      "acc": 0.5375,
      "acc_stderr": 0.03954089913497815,
      "acc_norm": 0.5375,
      "acc_norm_stderr": 0.03954089913497815
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-chatglm",
    "model_args": "pretrained='/home/sdk_models/chatglm2_6b',add_special_tokens=True,trust_remote_code=True,dtype='float16',use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:4",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "0:42:08.560191"
  }
}