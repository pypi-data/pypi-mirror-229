{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.3668639053254438,
      "acc_stderr": 0.03718316793853483,
      "acc_norm": 0.3668639053254438,
      "acc_norm_stderr": 0.03718316793853483
    },
    "Cmmlu-anatomy": {
      "acc": 0.27702702702702703,
      "acc_stderr": 0.036911647897386525,
      "acc_norm": 0.27702702702702703,
      "acc_norm_stderr": 0.036911647897386525
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.29878048780487804,
      "acc_stderr": 0.03585166336909662,
      "acc_norm": 0.29878048780487804,
      "acc_norm_stderr": 0.03585166336909662
    },
    "Cmmlu-arts": {
      "acc": 0.45625,
      "acc_stderr": 0.039500492593059405,
      "acc_norm": 0.45625,
      "acc_norm_stderr": 0.039500492593059405
    },
    "Cmmlu-astronomy": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.03477691162163659,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.03477691162163659
    },
    "Cmmlu-business_ethics": {
      "acc": 0.3827751196172249,
      "acc_stderr": 0.03370248274774052,
      "acc_norm": 0.3827751196172249,
      "acc_norm_stderr": 0.03370248274774052
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.38125,
      "acc_stderr": 0.03851802138867096,
      "acc_norm": 0.38125,
      "acc_norm_stderr": 0.03851802138867096
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.5038167938931297,
      "acc_stderr": 0.043851623256015534,
      "acc_norm": 0.5038167938931297,
      "acc_norm_stderr": 0.043851623256015534
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.35294117647058826,
      "acc_stderr": 0.04112975875177067,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.04112975875177067
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.4766355140186916,
      "acc_stderr": 0.048511241723296745,
      "acc_norm": 0.4766355140186916,
      "acc_norm_stderr": 0.048511241723296745
    },
    "Cmmlu-chinese_history": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.027825291191456943,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.027825291191456943
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.03198001660115071,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.03198001660115071
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.4748603351955307,
      "acc_stderr": 0.03742918386493423,
      "acc_norm": 0.4748603351955307,
      "acc_norm_stderr": 0.03742918386493423
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.31645569620253167,
      "acc_stderr": 0.030274974880218974,
      "acc_norm": 0.31645569620253167,
      "acc_norm_stderr": 0.030274974880218974
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.27358490566037735,
      "acc_stderr": 0.043505468189990605,
      "acc_norm": 0.27358490566037735,
      "acc_norm_stderr": 0.043505468189990605
    },
    "Cmmlu-college_education": {
      "acc": 0.5233644859813084,
      "acc_stderr": 0.048511241723296745,
      "acc_norm": 0.5233644859813084,
      "acc_norm_stderr": 0.048511241723296745
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.3584905660377358,
      "acc_stderr": 0.04679998780012862,
      "acc_norm": 0.3584905660377358,
      "acc_norm_stderr": 0.04679998780012862
    },
    "Cmmlu-college_law": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04557239513497751,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04557239513497751
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.3047619047619048,
      "acc_stderr": 0.045136767181683086,
      "acc_norm": 0.3047619047619048,
      "acc_norm_stderr": 0.045136767181683086
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.27358490566037735,
      "acc_stderr": 0.04350546818999061,
      "acc_norm": 0.27358490566037735,
      "acc_norm_stderr": 0.04350546818999061
    },
    "Cmmlu-college_medicine": {
      "acc": 0.30036630036630035,
      "acc_stderr": 0.027795629283121376,
      "acc_norm": 0.30036630036630035,
      "acc_norm_stderr": 0.027795629283121376
    },
    "Cmmlu-computer_science": {
      "acc": 0.4166666666666667,
      "acc_stderr": 0.03460228327239172,
      "acc_norm": 0.4166666666666667,
      "acc_norm_stderr": 0.03460228327239172
    },
    "Cmmlu-computer_security": {
      "acc": 0.3508771929824561,
      "acc_stderr": 0.03660298834049162,
      "acc_norm": 0.3508771929824561,
      "acc_norm_stderr": 0.03660298834049162
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.3673469387755102,
      "acc_stderr": 0.03989739969449137,
      "acc_norm": 0.3673469387755102,
      "acc_norm_stderr": 0.03989739969449137
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.28776978417266186,
      "acc_stderr": 0.03853836179233389,
      "acc_norm": 0.28776978417266186,
      "acc_norm_stderr": 0.03853836179233389
    },
    "Cmmlu-economics": {
      "acc": 0.3270440251572327,
      "acc_stderr": 0.0373222564649312,
      "acc_norm": 0.3270440251572327,
      "acc_norm_stderr": 0.0373222564649312
    },
    "Cmmlu-education": {
      "acc": 0.4110429447852761,
      "acc_stderr": 0.038656978537853624,
      "acc_norm": 0.4110429447852761,
      "acc_norm_stderr": 0.038656978537853624
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.3313953488372093,
      "acc_stderr": 0.03599646438179591,
      "acc_norm": 0.3313953488372093,
      "acc_norm_stderr": 0.03599646438179591
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.32936507936507936,
      "acc_stderr": 0.029665044009663515,
      "acc_norm": 0.32936507936507936,
      "acc_norm_stderr": 0.029665044009663515
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.43434343434343436,
      "acc_stderr": 0.03531505879359183,
      "acc_norm": 0.43434343434343436,
      "acc_norm_stderr": 0.03531505879359183
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.5378151260504201,
      "acc_stderr": 0.032385469487589795,
      "acc_norm": 0.5378151260504201,
      "acc_norm_stderr": 0.032385469487589795
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.26956521739130435,
      "acc_stderr": 0.02932276422894952,
      "acc_norm": 0.26956521739130435,
      "acc_norm_stderr": 0.02932276422894952
    },
    "Cmmlu-ethnology": {
      "acc": 0.4,
      "acc_stderr": 0.04232073695151589,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.04232073695151589
    },
    "Cmmlu-food_science": {
      "acc": 0.34965034965034963,
      "acc_stderr": 0.04001716028382394,
      "acc_norm": 0.34965034965034963,
      "acc_norm_stderr": 0.04001716028382394
    },
    "Cmmlu-genetics": {
      "acc": 0.3352272727272727,
      "acc_stderr": 0.03568512682153707,
      "acc_norm": 0.3352272727272727,
      "acc_norm_stderr": 0.03568512682153707
    },
    "Cmmlu-global_facts": {
      "acc": 0.4228187919463087,
      "acc_stderr": 0.040607137330584464,
      "acc_norm": 0.4228187919463087,
      "acc_norm_stderr": 0.040607137330584464
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.3254437869822485,
      "acc_stderr": 0.03614867847292204,
      "acc_norm": 0.3254437869822485,
      "acc_norm_stderr": 0.03614867847292204
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.22727272727272727,
      "acc_stderr": 0.03661433360410719,
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.03661433360410719
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.4067796610169492,
      "acc_stderr": 0.0454145170886159,
      "acc_norm": 0.4067796610169492,
      "acc_norm_stderr": 0.0454145170886159
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2926829268292683,
      "acc_stderr": 0.035637888362588285,
      "acc_norm": 0.2926829268292683,
      "acc_norm_stderr": 0.035637888362588285
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.04265792110940589,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.04265792110940589
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.4965034965034965,
      "acc_stderr": 0.04195804195804197,
      "acc_norm": 0.4965034965034965,
      "acc_norm_stderr": 0.04195804195804197
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04216370213557835,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04216370213557835
    },
    "Cmmlu-international_law": {
      "acc": 0.2864864864864865,
      "acc_stderr": 0.03333068663336698,
      "acc_norm": 0.2864864864864865,
      "acc_norm_stderr": 0.03333068663336698
    },
    "Cmmlu-journalism": {
      "acc": 0.436046511627907,
      "acc_stderr": 0.03792189197270774,
      "acc_norm": 0.436046511627907,
      "acc_norm_stderr": 0.03792189197270774
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.35523114355231145,
      "acc_stderr": 0.023635544656859724,
      "acc_norm": 0.35523114355231145,
      "acc_norm_stderr": 0.023635544656859724
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.6121495327102804,
      "acc_stderr": 0.033386517359181925,
      "acc_norm": 0.6121495327102804,
      "acc_norm_stderr": 0.033386517359181925
    },
    "Cmmlu-logical": {
      "acc": 0.4065040650406504,
      "acc_stderr": 0.04446941388965174,
      "acc_norm": 0.4065040650406504,
      "acc_norm_stderr": 0.04446941388965174
    },
    "Cmmlu-machine_learning": {
      "acc": 0.319672131147541,
      "acc_stderr": 0.04239540943837382,
      "acc_norm": 0.319672131147541,
      "acc_norm_stderr": 0.04239540943837382
    },
    "Cmmlu-management": {
      "acc": 0.37142857142857144,
      "acc_stderr": 0.03342272296374863,
      "acc_norm": 0.37142857142857144,
      "acc_norm_stderr": 0.03342272296374863
    },
    "Cmmlu-marketing": {
      "acc": 0.4388888888888889,
      "acc_stderr": 0.0370915696198558,
      "acc_norm": 0.4388888888888889,
      "acc_norm_stderr": 0.0370915696198558
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.4603174603174603,
      "acc_stderr": 0.03635121936293256,
      "acc_norm": 0.4603174603174603,
      "acc_norm_stderr": 0.03635121936293256
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3448275862068966,
      "acc_stderr": 0.044323074959803505,
      "acc_norm": 0.3448275862068966,
      "acc_norm_stderr": 0.044323074959803505
    },
    "Cmmlu-nutrition": {
      "acc": 0.33793103448275863,
      "acc_stderr": 0.0394170763206489,
      "acc_norm": 0.33793103448275863,
      "acc_norm_stderr": 0.0394170763206489
    },
    "Cmmlu-philosophy": {
      "acc": 0.3904761904761905,
      "acc_stderr": 0.04783832298114145,
      "acc_norm": 0.3904761904761905,
      "acc_norm_stderr": 0.04783832298114145
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.38857142857142857,
      "acc_stderr": 0.03695163610374524,
      "acc_norm": 0.38857142857142857,
      "acc_norm_stderr": 0.03695163610374524
    },
    "Cmmlu-professional_law": {
      "acc": 0.2890995260663507,
      "acc_stderr": 0.03128372390561387,
      "acc_norm": 0.2890995260663507,
      "acc_norm_stderr": 0.03128372390561387
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2553191489361702,
      "acc_stderr": 0.02251703243459229,
      "acc_norm": 0.2553191489361702,
      "acc_norm_stderr": 0.02251703243459229
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.4396551724137931,
      "acc_stderr": 0.03265711286547216,
      "acc_norm": 0.4396551724137931,
      "acc_norm_stderr": 0.03265711286547216
    },
    "Cmmlu-public_relations": {
      "acc": 0.40804597701149425,
      "acc_stderr": 0.0373659034133698,
      "acc_norm": 0.40804597701149425,
      "acc_norm_stderr": 0.0373659034133698
    },
    "Cmmlu-security_study": {
      "acc": 0.4222222222222222,
      "acc_stderr": 0.04266763404099582,
      "acc_norm": 0.4222222222222222,
      "acc_norm_stderr": 0.04266763404099582
    },
    "Cmmlu-sociology": {
      "acc": 0.40707964601769914,
      "acc_stderr": 0.03275266284786317,
      "acc_norm": 0.40707964601769914,
      "acc_norm_stderr": 0.03275266284786317
    },
    "Cmmlu-sports_science": {
      "acc": 0.38181818181818183,
      "acc_stderr": 0.03793713171165633,
      "acc_norm": 0.38181818181818183,
      "acc_norm_stderr": 0.03793713171165633
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.2972972972972973,
      "acc_stderr": 0.033695536918777164,
      "acc_norm": 0.2972972972972973,
      "acc_norm_stderr": 0.033695536918777164
    },
    "Cmmlu-virology": {
      "acc": 0.39644970414201186,
      "acc_stderr": 0.03773949997679295,
      "acc_norm": 0.39644970414201186,
      "acc_norm_stderr": 0.03773949997679295
    },
    "Cmmlu-world_history": {
      "acc": 0.39751552795031053,
      "acc_stderr": 0.038689221123968776,
      "acc_norm": 0.39751552795031053,
      "acc_norm_stderr": 0.038689221123968776
    },
    "Cmmlu-world_religions": {
      "acc": 0.3375,
      "acc_stderr": 0.03749999999999997,
      "acc_norm": 0.3375,
      "acc_norm_stderr": 0.03749999999999997
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-chatglm",
    "model_args": "pretrained='/data1/cgzhang6/models/chatglm-6b',add_special_tokens=True,trust_remote_code=True,dtype='float16',use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:04:08.638111"
  }
}